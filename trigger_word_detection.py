# -*- coding: utf-8 -*-
"""Trigger word detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/trigger-word-detection-6cc20289-9735-4c0e-a385-6b7e15b24dbc.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240828/auto/storage/goog4_request%26X-Goog-Date%3D20240828T183245Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db0c36ba12ad89af5b6acbaea0c5c113289194b1694824f18e30065414c7f1c9569bb5af6f633955982953961ad45874db8f11c5a8671ee28fc90c13f593f926fd81026d4bdc4df59be1b2f5381b64c8aae4cb9132726ef49e9accce85966ce9ddd6a53d0b707e6275577bbb9c66ee2dbc9966324bb0b034b50d72f2f79e5b56659ce82d097611959f21648feca66981bf67080e40342811a055646446b2d8b94b3ab7b67766e2b16cfc6c9b025b6d85d87c2f7995c8d2fca39e031393d0bdb4b503ca3305a2a45f64b5cb95ce0f70593f4634ecb86e78a974640ff13f01cc10299a2934106d79d20b135f39ba5ac410d59c5fc79ebfedfe7019b7c43cb2f1c91
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchaudio
import sys
import os
import numpy as np
import matplotlib.pyplot as plt
import IPython.display as ipd

from tqdm import tqdm
import torchaudio.transforms as T
from torch.utils.data import TensorDataset, DataLoader
import random
from torch.nn.utils import clip_grad_norm_
import librosa

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

dataset_speech = torchaudio.datasets.SPEECHCOMMANDS('./', download=True)

def data_extractor(dataset, num_samples=5000, validation_split=0.2):

    trigger_samples = []
    non_trigger_samples = []

    # Extract samples
    for i in range(num_samples):
        waveform, sample_rate, label, *rest = dataset[i]
        if label == 'backward':
            trigger_samples.append((waveform, sample_rate, label))
        else:
            non_trigger_samples.append((waveform, sample_rate, label))

    # Split data into training and validation sets
    num_val_trigger = int(len(trigger_samples) * validation_split)
    num_val_non_trigger = int(len(non_trigger_samples) * validation_split)

    train_trigger_samples = trigger_samples[num_val_trigger:]
    val_trigger_samples = trigger_samples[:num_val_trigger]

    train_non_trigger_samples = non_trigger_samples[num_val_non_trigger:]
    val_non_trigger_samples = non_trigger_samples[:num_val_non_trigger]

    return (train_trigger_samples, train_non_trigger_samples), (val_trigger_samples, val_non_trigger_samples)

(train_trigger_samples, train_non_trigger_samples), (val_trigger_samples, val_non_trigger_samples) = data_extractor(dataset_speech)

print(f"Number of train trigger samples: {len(train_trigger_samples)}")
print(f"Number of train non-trigger samples: {len(train_non_trigger_samples)}")
print(f'Number of val trigger samples: {len(val_trigger_samples)}')
print(f'Number of val non-trigger samples: {len(val_non_trigger_samples)}')

lab = train_non_trigger_samples[410]
lab

waveform, sample, label = train_trigger_samples[20]
waveform_np = waveform.numpy().squeeze() # turn it into 1D instead of 2D to plot

plt.figure(figsize=(10, 4))
plt.plot(np.linspace(0, len(waveform_np)/16000, len(waveform_np)), waveform_np)
plt.title(f"Waveform of the word: {label}")
plt.xlabel("Time (s)")
plt.ylabel("Amplitude")
plt.show()

ipd.Audio(waveform, rate=16000)

def generate_and_label_spectrogram(waveform, label, target_length=101, n_fft=400, win_len=400, hop_len=160, n_mels=128, sample_rate=16000):
    # Create Mel Spectrogram
    mel_spectrogram = T.MelSpectrogram(
        sample_rate=sample_rate,
        n_fft=n_fft,
        win_length=win_len,
        hop_length=hop_len,
        n_mels=n_mels
    )(waveform)

    # Convert to dB
    mel_spec_db = T.AmplitudeToDB()(mel_spectrogram)

    # Check the current number of frames in the spectrogram
    num_frames = mel_spec_db.shape[-1]

    # Pad the spectrogram if it is less than the target length
    if num_frames < target_length:
        # Calculate the amount of padding needed
        padding_size = target_length - num_frames
        # Create a padding tensor of zeros with the same number of channels as the spectrogram
        padding = torch.zeros((mel_spec_db.shape[0], mel_spec_db.shape[1], padding_size))
        # Concatenate the padding to the spectrogram along the time dimension (columns)
        mel_spec_db = torch.cat((mel_spec_db, padding), dim=2)

    # Generate labels based on the input label
    if label == 'backward':
        labels = np.ones((target_length,))
    else:
        labels = np.zeros((target_length,))

    return mel_spec_db, labels

train_spectrogram_data = []
train_labels_data = []
val_spectrogram_data = []
val_labels_data = []

for waveform, sample_rate, label in train_trigger_samples + train_non_trigger_samples:
    spec, lbls = generate_and_label_spectrogram(waveform, label)
    train_spectrogram_data.append(spec)
    train_labels_data.append(lbls)

for waveform, sample_rate, label in val_trigger_samples + val_non_trigger_samples:
    spec, lbls = generate_and_label_spectrogram(waveform, label)
    val_spectrogram_data.append(spec)
    val_labels_data.append(lbls)

print(np.array(train_spectrogram_data).shape)
print(np.array(train_labels_data).shape)
print(np.array(val_spectrogram_data).shape)
print(np.array(val_labels_data).shape)

class TriggerWordGRUModel(nn.Module):
    def __init__(self):
        super(TriggerWordGRUModel, self).__init__()
        # Convolutional layer processes the features along the time dimension
        self.conv1 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=5, padding=2)
        self.dropout = nn.Dropout(0.5)
        self.batch_norm = nn.BatchNorm1d(64)
        self.relu = nn.ReLU()

        # First GRU layer
        self.gru1 = nn.GRU(input_size=64, hidden_size=128, num_layers=1, batch_first=True)

        # Second GRU layer
        self.gru2 = nn.GRU(input_size=128, hidden_size=128, num_layers=1, batch_first=True)

        # Linear layers
        self.linear = nn.Linear(128, 1)  # Reducing dimension before final classification
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Correcting input shape to (batch, channels, time)
        x = x.transpose(1, 2)  # Now shape (batch, features, time)

        x = self.conv1(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        x = self.dropout(x)

        # Transpose for GRU (batch, time, features)
        x = x.transpose(1, 2)  # Now shape (batch, time, features)

        x, _ = self.gru1(x)
        x = self.dropout(x)

        x, _ = self.gru2(x)
        x = self.dropout(x)

        x = self.linear(x)
        x = self.sigmoid(x)

        return x.squeeze(-1)

model = TriggerWordGRUModel()

spectrogram_tensor = torch.stack([s.squeeze(0).transpose(0,1) for s in train_spectrogram_data]) # now [batch, time, frequenzy]
labels_tensor = torch.Tensor(train_labels_data)

#create dataset and loader
dataset = TensorDataset(spectrogram_tensor, labels_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)


# Assuming val_spectrogram_data and val_labels_data are already lists of tensors
val_spectrogram_tensor = torch.stack([s.squeeze(0).transpose(0,1) for s in val_spectrogram_data])
val_labels_tensor = torch.Tensor(val_labels_data)

# Create the validation dataset and dataloader
val_dataset = TensorDataset(val_spectrogram_tensor, val_labels_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)

def train_model(model, train_dataloader, val_dataloader, loss_fn, optimizer, epochs=15):
    for epoch in range(epochs):
        model.train()  # Set the model to training mode
        train_loss = 0
        for spectrograms, labels in train_dataloader:
            # Forward pass
            outputs = model(spectrograms)
            loss = loss_fn(outputs, labels)
            train_loss += loss.item()

            # Backward pass and optimize
            optimizer.zero_grad()
            loss.backward()
            clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to avoid explosion
            optimizer.step()

        # Validation phase
        model.eval()  # Set the model to evaluation mode
        val_loss = 0
        with torch.no_grad():  # Turn off gradients for validation to save memory and computations
            for spectrograms, labels in val_dataloader:
                outputs = model(spectrograms)
                loss = loss_fn(outputs, labels)
                val_loss += loss.item()

        # Print average losses for the current epoch
        train_loss /= len(train_dataloader)
        val_loss /= len(val_dataloader)
        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')

    print('Training Finished')

loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)
train_model(model, dataloader, val_dataloader, loss_fn, optimizer)

model.eval()

# Prepare the single spectrogram data example
# `spectrogram_data[0]` is assumed to be of shape [1, 128, 101] (1 for batch size)
single_spectrogram = val_spectrogram_tensor[20].unsqueeze(0)

# Check the shape
print("Shape of the input spectrogram:", single_spectrogram.shape)

# Ensure it's on the correct device
device = next(model.parameters()).device
single_spectrogram = single_spectrogram.to(device)

# Make prediction
with torch.no_grad():
    prediction = model(single_spectrogram)

print("Prediction output:", prediction)

print(np.array(single_spectrogram).shape)

threshold = 0.5
binary_predictions = (prediction > threshold).int()
print(binary_predictions)

single_label = val_labels_tensor[20]

single_label

def load_audio(file_path, target_sample_rate=16000):
  audio, sample_rate = librosa.load(file_path, sr=target_sample_rate)
  return audio, sample_rate

audio, sample_rate = load_audio('/content/ttsmaker-file-2024-8-28-21-17-14.mp3')

audio2, sample_rate2 = load_audio('/content/Recording (2).m4a')

def create_spectrogram(waveform, n_fft=400, win_len=400, hop_len=160, n_mels=128, sample_rate=16000):
    waveform = torch.tensor(waveform).float()
    if waveform.ndim == 1:  # Add channel dimension if mono
        waveform = waveform.unsqueeze(0)

    mel_spectrogram = T.MelSpectrogram(
        sample_rate=sample_rate,
        n_fft=n_fft,
        win_length=win_len,
        hop_length=hop_len,
        n_mels=n_mels
    )(waveform)

    mel_spec_db = T.AmplitudeToDB()(mel_spectrogram)
    return mel_spec_db

spectrogram = create_spectrogram(audio)
spectrogram = spectrogram.transpose(2,1)


model.eval()
device = next(model.parameters()).device
spectrogram = spectrogram.to(device)

with torch.no_grad():
    prediction1 = model(spectrogram)
    binary_predictions1 = (prediction1 > threshold).int()

print("Prediction:", binary_predictions1)

spectrogram2 = create_spectrogram(audio2)
spectrogram2 = spectrogram2.transpose(2,1)


model.eval()
device = next(model.parameters()).device
spectrogram2 = spectrogram2.to(device)

with torch.no_grad():
    prediction2 = model(spectrogram2)
    binary_predictions2 = (prediction2 > threshold).int()
print("Prediction:", binary_predictions2)

torch.save(model, 'Trigger_word_detection_model.pth')

